# INSTALL HOW TO
## preconditions
 - an AWS account to play with.
 - docker installed on local computer
 - either AWS credentials file or environment variables exist per https://www.terraform.io/docs/providers/aws/index.html#authentication
 - credentials have Admin access to ECR/VPC/ECS/EC2/CodePipeline/CodeBuild/S3 in default region
 - AWS_DEFAULT_REGION env var points to AWS region to play with
 - GITHUB_TOKEN env var points to GITHUB token for repo to checkout from

## installation 

```sh
export AWS_ACCESS_KEY_ID="accesskey"
export AWS_SECRET_ACCESS_KEY="secret"
export AWS_DEFAULT_REGION="us-west-2" # whatever region you play with
export GITHUB_TOKEN=86311d28af920249cca8ec8229cefaab1c21c4c8  
#seems AWS CodePipeline requires GitHib token even for public repos?
#See https://www.terraform.io/docs/providers/aws/r/codepipeline.html

# terraform bootstrap

cd terraform/live/dev/services
terraform init && terraform apply -auto-approve

```

# Test
- terraform/dev/services/terraform output prints DNS name of stack load balancer
- WAIT while CodePipeline pickups code from github repo,compiles and deploys to autogenerated terraform stack
- point your browser to the DNS name
- Note - CodePipeline polls github, no webhooks (those require Github ORG instead of personal account, too much hassle for the demo)


# tech/design notes

## VPC layout
- dedicated VPC 
- public and private networks, in multi AZs just for extensiblity/best pracices demo
## terraform state
- local file, for the sake of demo convenience
## terraform files layout
- modules //reusable infrastructure as code, no state, no AWS account/region specifics
- versioning - by git tags or by explicit module folder(s) rename
- modules/vpc // spinup VPC
-- modules/service // manage AWS ECS service
- live // root folder for STATEFUL environment(s). Separated AWS accounts per env(dev, vs qa vs prod)
-- live/dev // dev env
--live/qa // qa env
-- boostrap // to be run once, setups terraform shared remote state storage in S3
-- terraform/live/dev/secrets // secrets storage, intented be stored in SEPARATED git repo, encrypted. Wrapping shell code required, skipped for the demo, see notes
## secrets management
- SSL/ssh private keys, passwords,etc
- secure secrets storage, per ENV, protected by encryption or/and infrastructure(IP whitelist/etc). Authenthication/authorization/audit.Should we keep old versions of secrets? (balance between security and convenience) 
- how APP code should learn secrets? To minimize runtime dependencies/failure points, arguably deployment automation should "merge" app code and secrets at deploy time only. Better have deploy time failure than runtime failure (misconfgiured perms to AWS Secrets Manager,etc) . How to rotate secrets (explicitly redeploy same app version)?
- arguably, reuse gears and share same storage and tooling for infrastructure (ssh/ssl private keys,etc) and APP (database passwords,API KEYS,etc) secrets
- small dedicated git repo, that keeps encrypted files of "secrets" terraform module, checkoutable by privileged server/user only (CI server) and only for deploy time, referenced from  terraform infrastructure code. See terraform/live/dev/secrets    


## notes
-  ~~bad:chicken and egg for backend bootstrap. Want keep vital S3 bucket under control. Prob to have wrapping BAT/SHELL script for bootstrapping~~
-  ~~bad: backend config prohibits interpolation - have dups across code~~
- bad:terraform keeps secrets in plaintext in state files, have to protect those
- if have to create  service role forl ECS from terraform, set create_ecs_service_linked_role to true in terraform/modules/services/variables.tf
- terraforms stack pickups code from GitHub repo https://github.com/curiouscat2000/quest1 
- configurable in terraform/live/dev/services/terraform.tfvars 

